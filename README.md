# LLM-Powered Legal Queryâ€“Retrieval System ğŸ”ğŸ“„

A Streamlit-based intelligent assistant that answers queries about legal documents using LLMs and semantic search. It highlights relevant clauses in PDFs and explains them in context â€” powered by Gemini API.

---

## ğŸš€ Features

- ğŸ” **Semantic Clause Search** â€” Retrieves most relevant sections from a legal document using FAISS + embeddings.
- ğŸ“„ **PDF Clause Highlighting** â€” Highlights matched clauses in the original PDF using PyMuPDF.
- ğŸ’¡ **LLM Explanation** â€” Explains the retrieved clauses using Gemini/GPT APIs.
- ğŸ§  **Final Decision Inference** â€” Outputs a yes/no/maybe decision with reasoning.
- ğŸ“¤ **Downloadable Outputs** â€” Option to download structured JSON and annotated PDF.

---

## ğŸ–¥ï¸ Demo

Check out the live demo ğŸ‘‰ [Streamlit App](https://techbytesllmproject.streamlit.app/)

---

## ğŸ—‚ï¸ Folder Structure
```
llm_query_app/
â”œâ”€â”€ app.py # Streamlit UI and logic
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Project overview
â”œâ”€â”€ test_gemini.py # LLM test script
â”œâ”€â”€ data/
â”‚ â””â”€â”€ all_chunks.jsonl # Pre-processed document chunks
â”œâ”€â”€ vector_index/
â”‚ â””â”€â”€ faiss_index.index # Vector DB for retrieval
â””â”€â”€ utils/
â”œâ”€â”€ document_loader.py # Loads and processes documents
â”œâ”€â”€ vector_search.py # FAISS-based search
â”œâ”€â”€ llm_explainer.py # Gemini prompt and response logic
â”œâ”€â”€ pdf_highlighter.py # Highlights clauses in PDFs
â”œâ”€â”€ decision_engine.py # Decision logic based on LLM
```

---

## ğŸ› ï¸ Installation

```bash
# Clone the repo
git clone https://github.com/your-username/llm-query-retrieval.git
cd llm_query_app
```
---
### Create virtual environment (optional)
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```
---
### Install dependencies
```bash
pip install -r requirements.txt
```
---
### Run the app
```bash
streamlit run app.py
```
---
### ğŸ” Environment Setup
Create a .env file or set your environment variables for:

```ini
GEMINI_API_KEY=your_api_key_here
```
---

## ğŸ“„ Input/Output
Input: PDF document + user query

## Output:

Top matching clause(s)

Explanation by LLM

Final decision (Yes/No/Maybe)

Highlighted PDF download

Raw JSON output

---

## ğŸ§  LLM Backend
 Gemini Pro (via API)

 Token-level prompt customization

 Support for GPT-4 (optional future integration)

 ---

## ğŸ“¦ Future Improvements
Upload custom PDFs directly via UI

Add feedback/rating system per clause

Switchable models: GPT / Gemini / Ollama

MongoDB storage for user queries & analytics

---

## ğŸ‘¨â€ğŸ’» Contributors
Zahid Mohammed â€” Full Stack + AI Developer

---

